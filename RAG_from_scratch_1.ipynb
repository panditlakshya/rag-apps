{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26316bc3-c92e-4514-96ff-668b22df24e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain_community tiktoken langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b73f2ea-402e-4004-8ba6-936abac56626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02fc98a-a98c-49cf-bcc6-6e2594a24dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4b5a60-077c-48cb-a799-4ad630856730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e0ca3d-1625-46e6-9bce-4ae27bbc4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb98fe2-c843-431e-b523-0321ab897cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdae2c99-99da-4d22-97d0-950ed540f028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fed8995-3ba3-4dda-837a-8c94ffb30842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0abd25d4-259e-430c-af3a-400859840343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0dba56-eb30-4738-8a75-3ff545f8be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08480e04-a27b-4f1d-a844-c54efca15bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57631346-8d96-4f1a-8a42-b238a8d9ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97adcf17-a609-4886-ada8-fbc08a102b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 = '''You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:'''\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cee1129-da38-4d61-9a7d-3eb4fefb527e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down a complex task into smaller and simpler steps. This is done by instructing the model to \"think step by step\" and decomposing the task into manageable sub-tasks, allowing for more effective planning and execution.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65cba853-54a4-4944-a51a-25e99ec361ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f991dd63-39b1-4d89-97c6-1e34b6c6bef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28f57d62-d249-43e6-99c6-b003c0e27223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "310e5932-d4f6-49e5-ba0c-c7808486ea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8806581695120702\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b89e49f9-c266-437c-bb7c-b101a7480235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f16899da-f989-40cf-8da2-d957e0c46e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1}) # k=1, one doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85e5be88-1cc1-4a80-be25-d3154de06874",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7bc571b-caad-453d-8ad0-1d980d14cc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs),docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2587a33c-bc32-4d88-8dc1-cb6fc457e5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f0a8885-3357-49bd-9949-72a4e0aa38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8ca40f8-9d65-4ca2-9ecc-936d0f27760d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='According to the provided context, Task Decomposition is the process of breaking down a complicated task into smaller and simpler steps.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 334, 'total_tokens': 359, 'completion_time': 0.020833333, 'prompt_time': 0.075507598, 'queue_time': 0.018849735000000006, 'total_time': 0.096340931}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-0ee1bd83-aa3d-4b61-9123-9e6c85a91803-0', usage_metadata={'input_tokens': 334, 'output_tokens': 25, 'total_tokens': 359})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf2697ad-8a70-4d84-ae5d-6321d05fb654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the context, Task Decomposition is a process that involves breaking down a complicated task into smaller and simpler steps, allowing an agent to plan ahead and understand the task better.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "586dbd6a-a097-4b38-ac24-266c2c2201d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model|StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64709046-519c-415e-924f-d052a6de4f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, Task decomposition is the process of breaking down a complicated task into smaller and simpler steps. It is achieved through various techniques, including Chain of Thought (CoT), Tree of Thoughts (Yao et al. 2023), and simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", or using task-specific instructions.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\":docs,\"question\":\"What is Task decomposition\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
